{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Web Scraping Data Project Documentation","text":""},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"ETL/","title":"ETL","text":""},{"location":"ETL/#webscraper-class","title":"WebScraper class","text":"Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>class WebScraper:\n    def __init__(self, headless=False):\n        self.driver = self.initialize_webdriver(headless)\n\n    def initialize_webdriver(self, headless=False):\n        \"\"\"\n        Initializes the webdriver with the specified options.\n        - `headless`: `bool`, whether to run the browser in headless mode. Default is `False`.\n        This is useful for running the scraper in a headless environment, such as a server.\n        \"\"\"\n        options = Options()\n        if headless:\n            options.add_argument(\"--headless\")\n        driver = webdriver.Chrome(options=options)\n        return driver\n\n    def navigate_to_page(self, url):\n        \"\"\"Navigates to the specified page and returns the current URL.\"\"\"\n        self.driver.get(url)\n        return self.driver.current_url\n\n    def click_got_it_button(self):\n        \"\"\"\n        Clicks the \"Got it\" button if it exists. If it doesn't, continue with the process.\n        \"\"\"\n        try:\n            wait = WebDriverWait(self.driver, 10)  # Wait for 10 seconds.\n            got_it_button = wait.until(\n                EC.element_to_be_clickable((By.CSS_SELECTOR, \".cc-btn.cc-dismiss\"))\n            )\n            got_it_button.click()\n        except TimeoutException:\n            pass\n\n    def extract_table_data(self):\n        \"\"\"\n        Extracts the data from the table and returns a list of dictionaries.\n        \"\"\"\n        try:\n            wait = WebDriverWait(self.driver, 5)  # Wait for 10 seconds.\n            table = wait.until(\n                EC.presence_of_element_located(\n                    (\n                        By.XPATH,\n                        \"/html/body/div[2]/div[2]/main/div[2]/div/div/div/article/div/div/div/div/div/ul[2]\",\n                    )\n                )\n            )\n            rows = table.find_elements(By.TAG_NAME, \"li\")\n        except NoSuchElementException:\n            print(\"The table was not found.\")\n            return []\n\n        data = []\n        wait = WebDriverWait(self.driver, 5)  # Wait for 5 seconds.\n\n        for row in rows:\n            logo_url = row.find_element(\n                By.CSS_SELECTOR, \".m-exhibitors-list__items__item__logo__link img\"\n            ).get_attribute(\"src\")\n            name = row.find_element(\n                By.CSS_SELECTOR,\n                \".m-exhibitors-list__items__item__name .m-exhibitors-list__items__item__name__link\",\n            ).text.strip()\n            hall = row.find_element(\n                By.CSS_SELECTOR, \".m-exhibitors-list__items__item__hall\"\n            ).text.strip()\n            stand = row.find_element(\n                By.CSS_SELECTOR, \".m-exhibitors-list__items__item__stand\"\n            ).text.strip()\n            try:\n                category_icon_url = row.find_element(\n                    By.CSS_SELECTOR,\n                    \".m-exhibitors-list__items__item__category img\",\n                ).get_attribute(\"src\")\n            except NoSuchElementException:\n                category_icon_url = None\n            try:\n                country_element = row.find_element(\n                    By.XPATH,\n                    \".//div[contains(@class, 'm-exhibitors-list__items__item__location')]\",\n                )\n                country = country_element.text.strip()\n            except NoSuchElementException:\n                print(\"Country not found, waiting for 5 seconds\")\n                sleep(5)\n                country = None\n\n            row_data = {\n                \"logo_url\": logo_url,\n                \"name\": name,\n                \"hall\": hall,\n                \"stand\": stand,\n                \"category_icon_url\": category_icon_url,\n                \"country\": country,\n            }\n            data.append(row_data)\n\n        return data\n\n    def data_to_dataframe(self, data):\n        \"\"\"Converts the data to a pandas DataFrame.\"\"\"\n        df = pd.DataFrame(data)\n        return df\n\n    def export_to_csv(self, df, filename):\n        \"\"\"\n        Exports the DataFrame to a CSV file. Define the filename without the extension.\n        \"\"\"\n        df.to_csv(filename, index=False, header=True)\n\n    def extract_total_pages(self):\n        \"\"\"\n        Extracts the total number of pages from the pagination.\n        \"\"\"\n        element = self.driver.find_element(\n            By.XPATH,\n            \"/html/body/div[2]/div[2]/main/div[2]/div/div/div/article/div/div/div/div/div/div[6]/div/ul/li[8]/a\",\n        )\n        total_pages_text = element.text\n\n        return int(total_pages_text)\n\n    def close_driver(self):\n        \"\"\"Quits the webdriver.\"\"\"\n        self.driver.quit()\n</code></pre>"},{"location":"ETL/#app.scripts.web_scraper.WebScraper.click_got_it_button","title":"<code>click_got_it_button()</code>","text":"<p>Clicks the \"Got it\" button if it exists. If it doesn't, continue with the process.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def click_got_it_button(self):\n    \"\"\"\n    Clicks the \"Got it\" button if it exists. If it doesn't, continue with the process.\n    \"\"\"\n    try:\n        wait = WebDriverWait(self.driver, 10)  # Wait for 10 seconds.\n        got_it_button = wait.until(\n            EC.element_to_be_clickable((By.CSS_SELECTOR, \".cc-btn.cc-dismiss\"))\n        )\n        got_it_button.click()\n    except TimeoutException:\n        pass\n</code></pre>"},{"location":"ETL/#app.scripts.web_scraper.WebScraper.close_driver","title":"<code>close_driver()</code>","text":"<p>Quits the webdriver.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def close_driver(self):\n    \"\"\"Quits the webdriver.\"\"\"\n    self.driver.quit()\n</code></pre>"},{"location":"ETL/#app.scripts.web_scraper.WebScraper.data_to_dataframe","title":"<code>data_to_dataframe(data)</code>","text":"<p>Converts the data to a pandas DataFrame.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def data_to_dataframe(self, data):\n    \"\"\"Converts the data to a pandas DataFrame.\"\"\"\n    df = pd.DataFrame(data)\n    return df\n</code></pre>"},{"location":"ETL/#app.scripts.web_scraper.WebScraper.export_to_csv","title":"<code>export_to_csv(df, filename)</code>","text":"<p>Exports the DataFrame to a CSV file. Define the filename without the extension.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def export_to_csv(self, df, filename):\n    \"\"\"\n    Exports the DataFrame to a CSV file. Define the filename without the extension.\n    \"\"\"\n    df.to_csv(filename, index=False, header=True)\n</code></pre>"},{"location":"ETL/#app.scripts.web_scraper.WebScraper.extract_table_data","title":"<code>extract_table_data()</code>","text":"<p>Extracts the data from the table and returns a list of dictionaries.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def extract_table_data(self):\n    \"\"\"\n    Extracts the data from the table and returns a list of dictionaries.\n    \"\"\"\n    try:\n        wait = WebDriverWait(self.driver, 5)  # Wait for 10 seconds.\n        table = wait.until(\n            EC.presence_of_element_located(\n                (\n                    By.XPATH,\n                    \"/html/body/div[2]/div[2]/main/div[2]/div/div/div/article/div/div/div/div/div/ul[2]\",\n                )\n            )\n        )\n        rows = table.find_elements(By.TAG_NAME, \"li\")\n    except NoSuchElementException:\n        print(\"The table was not found.\")\n        return []\n\n    data = []\n    wait = WebDriverWait(self.driver, 5)  # Wait for 5 seconds.\n\n    for row in rows:\n        logo_url = row.find_element(\n            By.CSS_SELECTOR, \".m-exhibitors-list__items__item__logo__link img\"\n        ).get_attribute(\"src\")\n        name = row.find_element(\n            By.CSS_SELECTOR,\n            \".m-exhibitors-list__items__item__name .m-exhibitors-list__items__item__name__link\",\n        ).text.strip()\n        hall = row.find_element(\n            By.CSS_SELECTOR, \".m-exhibitors-list__items__item__hall\"\n        ).text.strip()\n        stand = row.find_element(\n            By.CSS_SELECTOR, \".m-exhibitors-list__items__item__stand\"\n        ).text.strip()\n        try:\n            category_icon_url = row.find_element(\n                By.CSS_SELECTOR,\n                \".m-exhibitors-list__items__item__category img\",\n            ).get_attribute(\"src\")\n        except NoSuchElementException:\n            category_icon_url = None\n        try:\n            country_element = row.find_element(\n                By.XPATH,\n                \".//div[contains(@class, 'm-exhibitors-list__items__item__location')]\",\n            )\n            country = country_element.text.strip()\n        except NoSuchElementException:\n            print(\"Country not found, waiting for 5 seconds\")\n            sleep(5)\n            country = None\n\n        row_data = {\n            \"logo_url\": logo_url,\n            \"name\": name,\n            \"hall\": hall,\n            \"stand\": stand,\n            \"category_icon_url\": category_icon_url,\n            \"country\": country,\n        }\n        data.append(row_data)\n\n    return data\n</code></pre>"},{"location":"ETL/#app.scripts.web_scraper.WebScraper.extract_total_pages","title":"<code>extract_total_pages()</code>","text":"<p>Extracts the total number of pages from the pagination.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def extract_total_pages(self):\n    \"\"\"\n    Extracts the total number of pages from the pagination.\n    \"\"\"\n    element = self.driver.find_element(\n        By.XPATH,\n        \"/html/body/div[2]/div[2]/main/div[2]/div/div/div/article/div/div/div/div/div/div[6]/div/ul/li[8]/a\",\n    )\n    total_pages_text = element.text\n\n    return int(total_pages_text)\n</code></pre>"},{"location":"ETL/#app.scripts.web_scraper.WebScraper.initialize_webdriver","title":"<code>initialize_webdriver(headless=False)</code>","text":"<p>Initializes the webdriver with the specified options. - <code>headless</code>: <code>bool</code>, whether to run the browser in headless mode. Default is <code>False</code>. This is useful for running the scraper in a headless environment, such as a server.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def initialize_webdriver(self, headless=False):\n    \"\"\"\n    Initializes the webdriver with the specified options.\n    - `headless`: `bool`, whether to run the browser in headless mode. Default is `False`.\n    This is useful for running the scraper in a headless environment, such as a server.\n    \"\"\"\n    options = Options()\n    if headless:\n        options.add_argument(\"--headless\")\n    driver = webdriver.Chrome(options=options)\n    return driver\n</code></pre>"},{"location":"ETL/#app.scripts.web_scraper.WebScraper.navigate_to_page","title":"<code>navigate_to_page(url)</code>","text":"<p>Navigates to the specified page and returns the current URL.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def navigate_to_page(self, url):\n    \"\"\"Navigates to the specified page and returns the current URL.\"\"\"\n    self.driver.get(url)\n    return self.driver.current_url\n</code></pre>"},{"location":"ETL/#transformer-class","title":"Transformer class","text":"<p>Class to transform the data. Performs a series of transformations such as: - concatenating the data - removing duplicates - resetting the index - extracting categories from urls</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>class Transformer:\n    \"\"\"\n    Class to transform the data. Performs a series of transformations such as:\n    - concatenating the data\n    - removing duplicates\n    - resetting the index\n    - extracting categories from urls\n    \"\"\"\n\n    def concatenate_csv_files(self, raw_data):\n        \"\"\"\n        Concatenate csv files. Set the working directory to the data directory to map the csv files.\n        \"\"\"\n        # Set the pattern to match all CSV files\n        pattern = f\"{raw_data}/*.csv\"\n        # Get a list of all CSV files in the working directory\n        csv_files = glob.glob(pattern)\n\n        # Create a list to store the DataFrames\n        df_list = []\n\n        # Loop through the list of CSV files\n        for csv_file in csv_files:\n            # Read each CSV into a DataFrame and append to the list\n            df = pd.read_csv(csv_file)\n            df_list.append(df)\n\n        concatenated_df = pd.concat(df_list, ignore_index=True)\n\n        return concatenated_df\n\n    def get_category_names_from_url(self, df, url_column):\n        \"\"\"\n        Extract the category names from the URL and create a new column in the DataFrame\n        \"\"\"\n        # Check if the URL is empty or None\n        mask = df[url_column].notnull()\n\n        # Split the URL into parts based on the \"/\" character\n        parts = df.loc[mask, url_column].str.split(\"/\", expand=True)\n\n        # Get the last part of the URL (after the last \"/\")\n        last_part = parts.iloc[:, -1]\n        # Split the last part of the URL on the \".\" character\n        last_part_parts = last_part.str.split(\".\", expand=True)\n\n        # Get the first part of the last part of the URL\n        category_name = last_part_parts.loc[:, 0]\n\n        # Create a new column in the DataFrame and assign the category names\n        df.loc[mask, \"category_name\"] = category_name\n\n        return df\n\n    def format_categorical_values(self, df, column, replacements):\n        \"\"\"\n        Replace column values with new values.\n        \"\"\"\n        df[column] = df[column].replace(replacements)\n        print(f\"\\nUnique values in the column '{column}' after formatting:\")\n        # Return rows with the replaced values.\n        replaced_rows = df.loc[\n            df[column].isin(replacements.values()),\n            [\"name\", \"hall\", \"stand\", \"category_name\"],\n        ]\n        print(f\"Rows with the replaced values: {replaced_rows}\")\n        return df\n\n    def remove_duplicates(self, df):\n        \"\"\"\n        Remove duplicates from the DataFrame\n        \"\"\"\n        # Check for duplicates\n        print(f\"Number of duplicates: {df.duplicated().sum()}\")\n        # Print rows with duplicates\n        print(df[df.duplicated()])\n        df.drop_duplicates(inplace=True)\n        # Reset the index\n        df.reset_index(drop=True, inplace=True)\n        # Check for duplicates again\n        print(f\"Number of duplicates after removing: {df.duplicated().sum()}\")\n        return df\n\n    def split_and_explode(self, df, columns, separator):\n        \"\"\"\n        Split columns using a separator and explode the resulting lists.\n        \"\"\"\n        # Check if the values for the columns have values with the separator. Print if True and return the rows with the values.\n        print(f\"Are there any values with the separator '{separator}' in the columns?\")\n        for column in columns:\n            if df[column].str.contains(separator).any():\n                print(\n                    f\"\\nYes, there are values with the separator '{separator}' in the column '{column}'. The rows are:/n\"\n                )\n                print(\n                    df[[\"name\", \"hall\", \"stand\"]].loc[\n                        df[column].str.contains(separator, na=False)\n                    ]\n                )\n\n                # Apply split to the specified column\n                print(\n                    f\"\\nSplitting the values in the column '{column}' using the separator '{separator}'.\"\n                )\n                df[column] = df[column].str.split(separator)\n\n                # Explode the DataFrame on the specified column\n                print(\n                    f\"\\nExploding the DataFrame on the column '{column}'. Then, stripping leading and trailing whitespace from the values.\"\n                )\n                df_exploded = df.explode(column)\n\n                # Strip leading and trailing whitespace from the values\n                df_exploded[column] = df_exploded[column].str.strip()\n\n                # Print the column values to check if the split and explode operations were successful.\n                print(f\"\\nColumn '{column}' values after splitting and exploding:\")\n                print(df_exploded[column].unique())\n\n                # Assign the exploded DataFrame back to df\n                df = df_exploded\n\n            else:\n                print(\n                    f\"\\nNo, there are no values with the separator '{separator}' in the columns.\"\n                )\n                pass\n\n        return df\n\n    def select_rearrange_columns(self, df, columns):\n        \"\"\"\n        Reorder the columns in the DataFrame.\n        \"\"\"\n        columns = [\"name\", \"category_name\", \"country\", \"hall\", \"stand\"]\n        df = df[columns]\n\n        return df\n</code></pre>"},{"location":"ETL/#app.scripts.transformer.Transformer.concatenate_csv_files","title":"<code>concatenate_csv_files(raw_data)</code>","text":"<p>Concatenate csv files. Set the working directory to the data directory to map the csv files.</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>def concatenate_csv_files(self, raw_data):\n    \"\"\"\n    Concatenate csv files. Set the working directory to the data directory to map the csv files.\n    \"\"\"\n    # Set the pattern to match all CSV files\n    pattern = f\"{raw_data}/*.csv\"\n    # Get a list of all CSV files in the working directory\n    csv_files = glob.glob(pattern)\n\n    # Create a list to store the DataFrames\n    df_list = []\n\n    # Loop through the list of CSV files\n    for csv_file in csv_files:\n        # Read each CSV into a DataFrame and append to the list\n        df = pd.read_csv(csv_file)\n        df_list.append(df)\n\n    concatenated_df = pd.concat(df_list, ignore_index=True)\n\n    return concatenated_df\n</code></pre>"},{"location":"ETL/#app.scripts.transformer.Transformer.format_categorical_values","title":"<code>format_categorical_values(df, column, replacements)</code>","text":"<p>Replace column values with new values.</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>def format_categorical_values(self, df, column, replacements):\n    \"\"\"\n    Replace column values with new values.\n    \"\"\"\n    df[column] = df[column].replace(replacements)\n    print(f\"\\nUnique values in the column '{column}' after formatting:\")\n    # Return rows with the replaced values.\n    replaced_rows = df.loc[\n        df[column].isin(replacements.values()),\n        [\"name\", \"hall\", \"stand\", \"category_name\"],\n    ]\n    print(f\"Rows with the replaced values: {replaced_rows}\")\n    return df\n</code></pre>"},{"location":"ETL/#app.scripts.transformer.Transformer.get_category_names_from_url","title":"<code>get_category_names_from_url(df, url_column)</code>","text":"<p>Extract the category names from the URL and create a new column in the DataFrame</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>def get_category_names_from_url(self, df, url_column):\n    \"\"\"\n    Extract the category names from the URL and create a new column in the DataFrame\n    \"\"\"\n    # Check if the URL is empty or None\n    mask = df[url_column].notnull()\n\n    # Split the URL into parts based on the \"/\" character\n    parts = df.loc[mask, url_column].str.split(\"/\", expand=True)\n\n    # Get the last part of the URL (after the last \"/\")\n    last_part = parts.iloc[:, -1]\n    # Split the last part of the URL on the \".\" character\n    last_part_parts = last_part.str.split(\".\", expand=True)\n\n    # Get the first part of the last part of the URL\n    category_name = last_part_parts.loc[:, 0]\n\n    # Create a new column in the DataFrame and assign the category names\n    df.loc[mask, \"category_name\"] = category_name\n\n    return df\n</code></pre>"},{"location":"ETL/#app.scripts.transformer.Transformer.remove_duplicates","title":"<code>remove_duplicates(df)</code>","text":"<p>Remove duplicates from the DataFrame</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>def remove_duplicates(self, df):\n    \"\"\"\n    Remove duplicates from the DataFrame\n    \"\"\"\n    # Check for duplicates\n    print(f\"Number of duplicates: {df.duplicated().sum()}\")\n    # Print rows with duplicates\n    print(df[df.duplicated()])\n    df.drop_duplicates(inplace=True)\n    # Reset the index\n    df.reset_index(drop=True, inplace=True)\n    # Check for duplicates again\n    print(f\"Number of duplicates after removing: {df.duplicated().sum()}\")\n    return df\n</code></pre>"},{"location":"ETL/#app.scripts.transformer.Transformer.select_rearrange_columns","title":"<code>select_rearrange_columns(df, columns)</code>","text":"<p>Reorder the columns in the DataFrame.</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>def select_rearrange_columns(self, df, columns):\n    \"\"\"\n    Reorder the columns in the DataFrame.\n    \"\"\"\n    columns = [\"name\", \"category_name\", \"country\", \"hall\", \"stand\"]\n    df = df[columns]\n\n    return df\n</code></pre>"},{"location":"ETL/#app.scripts.transformer.Transformer.split_and_explode","title":"<code>split_and_explode(df, columns, separator)</code>","text":"<p>Split columns using a separator and explode the resulting lists.</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>def split_and_explode(self, df, columns, separator):\n    \"\"\"\n    Split columns using a separator and explode the resulting lists.\n    \"\"\"\n    # Check if the values for the columns have values with the separator. Print if True and return the rows with the values.\n    print(f\"Are there any values with the separator '{separator}' in the columns?\")\n    for column in columns:\n        if df[column].str.contains(separator).any():\n            print(\n                f\"\\nYes, there are values with the separator '{separator}' in the column '{column}'. The rows are:/n\"\n            )\n            print(\n                df[[\"name\", \"hall\", \"stand\"]].loc[\n                    df[column].str.contains(separator, na=False)\n                ]\n            )\n\n            # Apply split to the specified column\n            print(\n                f\"\\nSplitting the values in the column '{column}' using the separator '{separator}'.\"\n            )\n            df[column] = df[column].str.split(separator)\n\n            # Explode the DataFrame on the specified column\n            print(\n                f\"\\nExploding the DataFrame on the column '{column}'. Then, stripping leading and trailing whitespace from the values.\"\n            )\n            df_exploded = df.explode(column)\n\n            # Strip leading and trailing whitespace from the values\n            df_exploded[column] = df_exploded[column].str.strip()\n\n            # Print the column values to check if the split and explode operations were successful.\n            print(f\"\\nColumn '{column}' values after splitting and exploding:\")\n            print(df_exploded[column].unique())\n\n            # Assign the exploded DataFrame back to df\n            df = df_exploded\n\n        else:\n            print(\n                f\"\\nNo, there are no values with the separator '{separator}' in the columns.\"\n            )\n            pass\n\n    return df\n</code></pre>"}]}