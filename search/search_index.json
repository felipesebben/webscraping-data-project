{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Web Scraping Data Project Documentation","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to the main page of my web scraping project! If you're reading this, I'm glad that you took your sweet time to go through my work!</p>"},{"location":"#the-project-layout","title":"The project layout","text":"<pre><code>.github/\n.vscode/\napp/\n    etl/\n        extract.py\n        transform.py\n        load.py\n    scripts/\n        transformer.py\n        web_scraper.py\n    main.py\ndata/\ndocs/\ntests/\n    test_webscraping.py\n.gitignore\n.pre-commit-config.yaml\n.python-version\nmkdocs.yml\npoetry.lock\npyproject.toml\nREADME.md\n</code></pre>"},{"location":"#the-context","title":"The context","text":"<p>Every year, I web scrape data for upcoming food exhibitions. In this case, it is for Gulfood 2024. Usually, event organizers display a list of exhibitors that, albeit useful, do not serve some purposes. Why? - Because there are hundreds of exhibitors, the list is paginated.     - This is not a very useful resource if you are going through many exhibitors! - You rely on a good internet connection, as you need to access each page online.     - If you are sending people to an international fair, you need to be thoughtful about roaming! - You cannot filter more than one exhibitor that easily.</p> <p>In my case, I had to consider the scenario in which the data would be used: - The user: Businessmen, with no IT acumen, that need to know where to meet specific exhibitors.     - They would have limited access to the internet.     - They need to know beforehand what to discuss with the exhibitors without resorting to notebooks.     - They need to know the name of the people with whom they need to meet (and not rely on their memory alone).     - They may want to explore some potential business opportunities by looking for countries with the most number of exhibitors by each food sector. - The place: A crowded food exhibition, with lots of people to meet and no time to take notes. - The problem: How track down all clients and partners without relying on the webpage's problematic list while providing annotations and some basic information on the number of exhibitors according to country and sector?</p>"},{"location":"#the-solution-the-exhibitor-finder","title":"The solution - The Exhibitor Finder","text":"<p>I came up with a Exhibitor Finder using Tableau, a BI software that offers many solutions within its resources. Although Tableau is often used to produce dashboards and reports, I have noticed in the past that users welcome solutions to problems that they had not necessarily thought before. This is one of the cases. Before, our coworkers had to manually prepare a list of people they would like to meet, together with the main topics of discussion. Considering the aforementioned scenario, this could be tiresome or ineffective. Now, with this visualization, they can track down where they need to go, as well, as what to discuss and to whom they should talk.</p>"},{"location":"#1-the-web-scraping","title":"1. The Web Scraping","text":"<p>Here's an image with the first row of exhibitors as shown on their website:</p> <p></p> <p>As we can see, the table consists of the following fields: - <code>logo</code>: The logo of the exhibitor. - <code>Exhibitor Name</code>: The name of the exhibitor. - <code>Hall</code>: The hall(s) in which the exhibitor is located. - <code>Stand</code>: The stand(s) in which the exibitor is located. - <code>Find the Stand</code>: A button that takes the user to an interactive map showing the precise location of the exhibitor. - <code>Sectors</code>: An image that informs the exhibitor's sector in the fair. - <code>Country</code>: The country of origin of the exhibitor. - <code>Media Logos</code>: URLs that provide more textual and/or interactive information on each exhibitor.</p> <p>We were interested on extracting all but the last field.</p>"},{"location":"#what-were-some-of-the-challenges","title":"What were some of the challenges?","text":"<ol> <li>Dynamic content. No <code>BeautifulSoup</code> here - we gotta act like grownups and face the reality that today, static websites are on the low, while everything is dynamically generated! Hence, we summon <code>selenium</code> for the job!</li> <li>Handling a dynamic pagination. I noticed that the number of pages increased through the days. Iterating through each page required counting beforehand the total number of pages!</li> <li>Handling exceptions, empty rows, and other unexpected content. Web scraping is addictive but tiresome! We had to create ways to handle empty rows and other issues such as unrecognized HTML/CSS elements.</li> <li>Iterating through dozens of pages without crashing your app. Randomly, the automation would fail or crash. As such, we decided to export each page individually as a .csv file to be concatenated later on.</li> </ol> <p>Fortunately, we could manage to do the whole thing!</p>"},{"location":"#2-the-tests","title":"2. The tests","text":"<p>This project was designed only to improve. That may sound obvious, but in software development, maintaining quality is very relevant, an often neglected aspect in data projects. As such, we created some tests to make sure that the classes and their respective functions worked properly. We acknowledge that there much more testing could be done, and we expect to develop some more of them in the near future. Part of these tests were created using the Test-Driven Development approach. First, we wrote simple tests before writing any actual code. The test will inevitably fail. Then, we wrote the simplest possible code. The test should then pass. Later on, we refactor the tests and the code, only then aiming at effectiveness and other issues.</p>"},{"location":"#3-the-transformation","title":"3. The transformation","text":"<p>Preppin' the data was one of the most challening parts. There were a few difficulties!</p>"},{"location":"#what-were-some-of-the-challenges_1","title":"What were some of the challenges?","text":"<ol> <li>Concatenating the data.</li> <li>Getting the categorial names from URLS.</li> <li>Splitting and exploding rows: here, there were some fields - hall and stand - in which sometimes there were many values tied together with commas. If ignored, Tableau would read, for instance, <code>Hall 1, Hall 2</code> as a single different category. This would affect the user experience and the overall functionality of the product.</li> </ol>"},{"location":"#4-pre-commits-continuous-integration","title":"4. Pre-commits &amp; Continuous Integration","text":"<p>Finally, this project also implemented several resources aimed at quality control. The goal here is to create an environment in which any changes in development are not sent straight away to the main repository, therefore potentially affecting the product. - <code>pre-commit</code> hooks were created to prevent us from sending bad code already to the repository. We implemented formatting tools and hooks that avoid the sending of large files. - Working on branches. As a good practice, we tried to develop different part of our product in different branches. <code>webscraping</code>, for instance, took care of the earlier stages of the product, while <code>transformation</code> implemented the data transformation processes, and so on. - <code>CI routines</code> were implemented too. When pushing branches and making pull requests, we implemented this extra barrier to protect merging the main with bad quality code. Steps such as testing, pre-commit, among others, were used.</p>"},{"location":"#final-thoughts","title":"Final thoughts","text":"<p>Please, feel free to take a look at my code in the other pages of this documentation. I will make available a Tableau Public version of my visualization, so that you can take a look at the final result. Thank you for your time, and feel free to contact me!</p> <p>Contact: felipesebben@yahoo.com.br</p>"},{"location":"instructions/","title":"Instructions","text":""},{"location":"instructions/#installation-and-setup","title":"Installation and Setup","text":"<ol> <li>Clone the repo.</li> </ol> <pre><code>git clone https://github.com/felipesebben/webscraping-data-project.git\ncd webscraping-data-project\n</code></pre> <ol> <li>Configure the correct Python version with <code>pyenv</code>. If you don't have it installed, make sure you have the right Python version in your machine (<code>3.11.5</code>).</li> </ol> <pre><code>pyenv install 3.11.5\npyenv local 3.11.5\n</code></pre> <ol> <li>Install the project dependencies. This project used <code>poetry</code>:</li> </ol> <pre><code># install dependencies\npoetry install\n# run the created environment\npoetry shell\n</code></pre> <ol> <li>To run the tests:</li> </ol> <pre><code>task test\n</code></pre> <ol> <li>To run the app:</li> </ol> <pre><code>task run\n</code></pre> <p>If you want to see the automation working pass the argument <code>headless</code> as <code>False</code>.</p> <pre><code># app/etl/extract.py\ndef extract():\n    \"\"\"\n    Main function to run the extraction process.\n    \"\"\"\n    # Pass argument as false. Default is True\n    scraper = WebScraper(headless=False)\n</code></pre>"},{"location":"main_etl/","title":"ETL steps","text":""},{"location":"main_etl/#introduction","title":"Introduction","text":"<p>In this session, you can take a look at how the ETL classes are implemented in <code>main.py</code>.</p>"},{"location":"main_etl/#1-extraction","title":"1. Extraction","text":""},{"location":"main_etl/#app.etl.extract.extract","title":"<code>extract()</code>","text":"<p>Main function to run the extraction process.</p> Source code in <code>app\\etl\\extract.py</code> <pre><code>def extract():\n    \"\"\"\n    Main function to run the extraction process.\n    \"\"\"\n    # Create a new instance of the WebScraper class\n    scraper = WebScraper(headless=True)\n    # Navigate to the page\n    scraper.navigate_to_page(\"https://www.gulfood.com/exhibitors?&amp;page=01\")\n    sleep(5)\n    # # Click the \"Got it\" button\n    scraper.click_got_it_button()\n    # sleep(2)\n\n    # Get the total number of pages to iterate over\n    total_pages = scraper.extract_total_pages()\n\n    # Iterate over each page\n    for page_number in range(1, total_pages + 1):\n        url = f\"https://www.gulfood.com/exhibitors?&amp;page={page_number:02d}\"\n\n        scraper.navigate_to_page(url)\n        sleep(2)\n        scraper.click_got_it_button()\n        sleep(2)\n        data = scraper.extract_table_data()\n        df = scraper.data_to_dataframe(data)\n        print(df.head())\n        filename = f\"data/raw/gulfood_exhibitors_page_{page_number:02d}.csv\"\n        scraper.export_to_csv(df, filename)\n\n    scraper.close_driver()\n</code></pre>"},{"location":"main_etl/#2-transform","title":"2. Transform","text":""},{"location":"main_etl/#app.etl.transform.transform","title":"<code>transform()</code>","text":"<p>Main function to perform the transformation step. Calls the transformer class to perform the transformation.</p> Source code in <code>app\\etl\\transform.py</code> <pre><code>def transform():\n    \"\"\"\n    Main function to perform the transformation step. Calls the transformer class to perform the transformation.\n    \"\"\"\n    transformer = Transformer()\n    concatenated_data = transformer.concatenate_csv_files(\"data/raw/\")\n    transformed_data = transformer.get_category_names_from_url(\n        concatenated_data, \"category_icon_url\"\n    )\n\n    category_replacements = {\n        \"worldfood\": \"World Food\",\n        \"pulsesgrainscereals\": \"Pulses, Grains &amp; Cereals\",\n        \"dairy\": \"Dairy\",\n        \"beverages\": \"Beverages\",\n        \"meatpoultry\": \"Meat &amp; Poultry\",\n        \"fatsoils\": \"Fats &amp; Oils\",\n        \"powerbrands\": \"Power Brands\",\n    }\n    formatted_data = transformer.format_categorical_values(\n        transformed_data, \"category_name\", replacements=category_replacements\n    )\n    removed_duplicates = transformer.remove_duplicates(formatted_data)\n\n    exploded_data = transformer.split_and_explode(\n        removed_duplicates, [\"hall\", \"stand\"], \",\"\n    )\n\n    hall_replacements = {\n        \"Trade Center Arena\": \"Trade Centre Arena\",\n        \"Skh Rashid Hall\": \"Shk Rashid Hall\",\n        \"Hall2\": \"Hall 2\",\n    }\n    formatted_col_value = transformer.format_categorical_values(\n        exploded_data, \"hall\", replacements=hall_replacements\n    )\n\n    cols = [\"name\", \"category_name\", \"hall\", \"stand\"]\n    selected_cols = transformer.select_rearrange_columns(formatted_col_value, cols)\n    return selected_cols\n</code></pre>"},{"location":"main_etl/#3-load","title":"3. Load","text":""},{"location":"main_etl/#app.etl.load.load","title":"<code>load(transformed_data, file_path, file_type, file_name)</code>","text":"<p>Perform the load step. Save the transformed data to a defined format file. Parameters:</p> <ul> <li><code>transformed_data</code>: The transformed data to load</li> <li><code>file_path</code>: The path to save the file</li> <li><code>file_type</code>: The file type to save the file as (csv or xlsx)</li> <li><code>file_name</code>: The name of the file to save</li> </ul> Source code in <code>app\\etl\\load.py</code> <pre><code>def load(transformed_data, file_path, file_type, file_name):\n    \"\"\"\n    Perform the load step. Save the transformed data to a defined format file.\n    Parameters:\n\n    - `transformed_data`: The transformed data to load\n    - `file_path`: The path to save the file\n    - `file_type`: The file type to save the file as (csv or xlsx)\n    - `file_name`: The name of the file to save\n    \"\"\"\n    transformed_data = transform.transform()\n    # Create the directory if it doesn't exist\n    os.makedirs(file_path, exist_ok=True)\n    if file_type == \"csv\":\n        transformed_data.to_csv(f\"{file_path}/{file_name}\", index=False, header=True)\n    elif file_type == \"xlsx\":\n        transformed_data.to_excel(f\"{file_path}/{file_name}\", index=False, header=True)\n    else:\n        raise ValueError(\"File type not supported\")\n</code></pre>"},{"location":"scripts/","title":"ETL Classes and Functions","text":""},{"location":"scripts/#introduction","title":"Introduction","text":"<p>In this session, you will find the classes that were created to perform the extraction and the transformation stages of the ETL process. The former is a <code>WebScraper</code> class that handles all the steps taken to extract the data from the website. The later, <code>Transformer</code>, takes care of all processes used to get the data in shape and make it ready for analysis.</p>"},{"location":"scripts/#webscraper-class","title":"WebScraper class","text":"Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>class WebScraper:\n    def __init__(self, headless=False):\n        self.driver = self.initialize_webdriver(headless)\n\n    def initialize_webdriver(self, headless=False):\n        \"\"\"\n        Initializes the webdriver with the specified options.\n        - `headless`: `bool`, whether to run the browser in headless mode. Default is `False`.\n        This is useful for running the scraper in a headless environment, such as a server.\n        \"\"\"\n        options = Options()\n        if headless:\n            options.add_argument(\"--headless\")\n        driver = webdriver.Chrome(options=options)\n        return driver\n\n    def navigate_to_page(self, url):\n        \"\"\"Navigates to the specified page and returns the current URL.\"\"\"\n        self.driver.get(url)\n        return self.driver.current_url\n\n    def click_got_it_button(self):\n        \"\"\"\n        Clicks the \"Got it\" button if it exists. If it doesn't, continue with the process.\n        \"\"\"\n        try:\n            wait = WebDriverWait(self.driver, 10)  # Wait for 10 seconds.\n            got_it_button = wait.until(\n                EC.element_to_be_clickable((By.CSS_SELECTOR, \".cc-btn.cc-dismiss\"))\n            )\n            got_it_button.click()\n        except TimeoutException:\n            pass\n\n    def extract_table_data(self):\n        \"\"\"\n        Extracts the data from the table and returns a list of dictionaries.\n        \"\"\"\n        try:\n            wait = WebDriverWait(self.driver, 5)  # Wait for 10 seconds.\n            table = wait.until(\n                EC.presence_of_element_located(\n                    (\n                        By.XPATH,\n                        \"/html/body/div[2]/div[2]/main/div[2]/div/div/div/article/div/div/div/div/div/ul[2]\",\n                    )\n                )\n            )\n            rows = table.find_elements(By.TAG_NAME, \"li\")\n        except NoSuchElementException:\n            print(\"The table was not found.\")\n            return []\n\n        data = []\n        wait = WebDriverWait(self.driver, 5)  # Wait for 5 seconds.\n\n        for row in rows:\n            logo_url = row.find_element(\n                By.CSS_SELECTOR, \".m-exhibitors-list__items__item__logo__link img\"\n            ).get_attribute(\"src\")\n            name = row.find_element(\n                By.CSS_SELECTOR,\n                \".m-exhibitors-list__items__item__name .m-exhibitors-list__items__item__name__link\",\n            ).text.strip()\n            hall = row.find_element(\n                By.CSS_SELECTOR, \".m-exhibitors-list__items__item__hall\"\n            ).text.strip()\n            stand = row.find_element(\n                By.CSS_SELECTOR, \".m-exhibitors-list__items__item__stand\"\n            ).text.strip()\n            try:\n                category_icon_url = row.find_element(\n                    By.CSS_SELECTOR,\n                    \".m-exhibitors-list__items__item__category img\",\n                ).get_attribute(\"src\")\n            except NoSuchElementException:\n                category_icon_url = None\n            try:\n                country_element = row.find_element(\n                    By.XPATH,\n                    \".//div[contains(@class, 'm-exhibitors-list__items__item__location')]\",\n                )\n                country = country_element.text.strip()\n            except NoSuchElementException:\n                print(\"Country not found, waiting for 5 seconds\")\n                sleep(5)\n                country = None\n\n            row_data = {\n                \"logo_url\": logo_url,\n                \"name\": name,\n                \"hall\": hall,\n                \"stand\": stand,\n                \"category_icon_url\": category_icon_url,\n                \"country\": country,\n            }\n            data.append(row_data)\n\n        return data\n\n    def data_to_dataframe(self, data):\n        \"\"\"Converts the data to a pandas DataFrame.\"\"\"\n        df = pd.DataFrame(data)\n        return df\n\n    def export_to_csv(self, df, filename):\n        \"\"\"\n        Exports the DataFrame to a CSV file. Define the filename without the extension.\n        \"\"\"\n        df.to_csv(filename, index=False, header=True)\n\n    def extract_total_pages(self):\n        \"\"\"\n        Extracts the total number of pages from the pagination.\n        \"\"\"\n        element = self.driver.find_element(\n            By.XPATH,\n            \"/html/body/div[2]/div[2]/main/div[2]/div/div/div/article/div/div/div/div/div/div[6]/div/ul/li[8]/a\",\n        )\n        total_pages_text = element.text\n\n        return int(total_pages_text)\n\n    def close_driver(self):\n        \"\"\"Quits the webdriver.\"\"\"\n        self.driver.quit()\n</code></pre>"},{"location":"scripts/#app.scripts.web_scraper.WebScraper.click_got_it_button","title":"<code>click_got_it_button()</code>","text":"<p>Clicks the \"Got it\" button if it exists. If it doesn't, continue with the process.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def click_got_it_button(self):\n    \"\"\"\n    Clicks the \"Got it\" button if it exists. If it doesn't, continue with the process.\n    \"\"\"\n    try:\n        wait = WebDriverWait(self.driver, 10)  # Wait for 10 seconds.\n        got_it_button = wait.until(\n            EC.element_to_be_clickable((By.CSS_SELECTOR, \".cc-btn.cc-dismiss\"))\n        )\n        got_it_button.click()\n    except TimeoutException:\n        pass\n</code></pre>"},{"location":"scripts/#app.scripts.web_scraper.WebScraper.close_driver","title":"<code>close_driver()</code>","text":"<p>Quits the webdriver.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def close_driver(self):\n    \"\"\"Quits the webdriver.\"\"\"\n    self.driver.quit()\n</code></pre>"},{"location":"scripts/#app.scripts.web_scraper.WebScraper.data_to_dataframe","title":"<code>data_to_dataframe(data)</code>","text":"<p>Converts the data to a pandas DataFrame.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def data_to_dataframe(self, data):\n    \"\"\"Converts the data to a pandas DataFrame.\"\"\"\n    df = pd.DataFrame(data)\n    return df\n</code></pre>"},{"location":"scripts/#app.scripts.web_scraper.WebScraper.export_to_csv","title":"<code>export_to_csv(df, filename)</code>","text":"<p>Exports the DataFrame to a CSV file. Define the filename without the extension.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def export_to_csv(self, df, filename):\n    \"\"\"\n    Exports the DataFrame to a CSV file. Define the filename without the extension.\n    \"\"\"\n    df.to_csv(filename, index=False, header=True)\n</code></pre>"},{"location":"scripts/#app.scripts.web_scraper.WebScraper.extract_table_data","title":"<code>extract_table_data()</code>","text":"<p>Extracts the data from the table and returns a list of dictionaries.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def extract_table_data(self):\n    \"\"\"\n    Extracts the data from the table and returns a list of dictionaries.\n    \"\"\"\n    try:\n        wait = WebDriverWait(self.driver, 5)  # Wait for 10 seconds.\n        table = wait.until(\n            EC.presence_of_element_located(\n                (\n                    By.XPATH,\n                    \"/html/body/div[2]/div[2]/main/div[2]/div/div/div/article/div/div/div/div/div/ul[2]\",\n                )\n            )\n        )\n        rows = table.find_elements(By.TAG_NAME, \"li\")\n    except NoSuchElementException:\n        print(\"The table was not found.\")\n        return []\n\n    data = []\n    wait = WebDriverWait(self.driver, 5)  # Wait for 5 seconds.\n\n    for row in rows:\n        logo_url = row.find_element(\n            By.CSS_SELECTOR, \".m-exhibitors-list__items__item__logo__link img\"\n        ).get_attribute(\"src\")\n        name = row.find_element(\n            By.CSS_SELECTOR,\n            \".m-exhibitors-list__items__item__name .m-exhibitors-list__items__item__name__link\",\n        ).text.strip()\n        hall = row.find_element(\n            By.CSS_SELECTOR, \".m-exhibitors-list__items__item__hall\"\n        ).text.strip()\n        stand = row.find_element(\n            By.CSS_SELECTOR, \".m-exhibitors-list__items__item__stand\"\n        ).text.strip()\n        try:\n            category_icon_url = row.find_element(\n                By.CSS_SELECTOR,\n                \".m-exhibitors-list__items__item__category img\",\n            ).get_attribute(\"src\")\n        except NoSuchElementException:\n            category_icon_url = None\n        try:\n            country_element = row.find_element(\n                By.XPATH,\n                \".//div[contains(@class, 'm-exhibitors-list__items__item__location')]\",\n            )\n            country = country_element.text.strip()\n        except NoSuchElementException:\n            print(\"Country not found, waiting for 5 seconds\")\n            sleep(5)\n            country = None\n\n        row_data = {\n            \"logo_url\": logo_url,\n            \"name\": name,\n            \"hall\": hall,\n            \"stand\": stand,\n            \"category_icon_url\": category_icon_url,\n            \"country\": country,\n        }\n        data.append(row_data)\n\n    return data\n</code></pre>"},{"location":"scripts/#app.scripts.web_scraper.WebScraper.extract_total_pages","title":"<code>extract_total_pages()</code>","text":"<p>Extracts the total number of pages from the pagination.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def extract_total_pages(self):\n    \"\"\"\n    Extracts the total number of pages from the pagination.\n    \"\"\"\n    element = self.driver.find_element(\n        By.XPATH,\n        \"/html/body/div[2]/div[2]/main/div[2]/div/div/div/article/div/div/div/div/div/div[6]/div/ul/li[8]/a\",\n    )\n    total_pages_text = element.text\n\n    return int(total_pages_text)\n</code></pre>"},{"location":"scripts/#app.scripts.web_scraper.WebScraper.initialize_webdriver","title":"<code>initialize_webdriver(headless=False)</code>","text":"<p>Initializes the webdriver with the specified options. - <code>headless</code>: <code>bool</code>, whether to run the browser in headless mode. Default is <code>False</code>. This is useful for running the scraper in a headless environment, such as a server.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def initialize_webdriver(self, headless=False):\n    \"\"\"\n    Initializes the webdriver with the specified options.\n    - `headless`: `bool`, whether to run the browser in headless mode. Default is `False`.\n    This is useful for running the scraper in a headless environment, such as a server.\n    \"\"\"\n    options = Options()\n    if headless:\n        options.add_argument(\"--headless\")\n    driver = webdriver.Chrome(options=options)\n    return driver\n</code></pre>"},{"location":"scripts/#app.scripts.web_scraper.WebScraper.navigate_to_page","title":"<code>navigate_to_page(url)</code>","text":"<p>Navigates to the specified page and returns the current URL.</p> Source code in <code>app\\scripts\\web_scraper.py</code> <pre><code>def navigate_to_page(self, url):\n    \"\"\"Navigates to the specified page and returns the current URL.\"\"\"\n    self.driver.get(url)\n    return self.driver.current_url\n</code></pre>"},{"location":"scripts/#transformer-class","title":"Transformer class","text":"<p>Class to transform the data. Performs a series of transformations such as: - concatenating the data - removing duplicates - resetting the index - extracting categories from urls</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>class Transformer:\n    \"\"\"\n    Class to transform the data. Performs a series of transformations such as:\n    - concatenating the data\n    - removing duplicates\n    - resetting the index\n    - extracting categories from urls\n    \"\"\"\n\n    def concatenate_csv_files(self, raw_data):\n        \"\"\"\n        Concatenate csv files. Set the working directory to the data directory to map the csv files.\n        \"\"\"\n        # Set the pattern to match all CSV files\n        pattern = f\"{raw_data}/*.csv\"\n        # Get a list of all CSV files in the working directory\n        csv_files = glob.glob(pattern)\n\n        # Create a list to store the DataFrames\n        df_list = []\n\n        # Loop through the list of CSV files\n        for csv_file in csv_files:\n            # Read each CSV into a DataFrame and append to the list\n            df = pd.read_csv(csv_file)\n            df_list.append(df)\n\n        concatenated_df = pd.concat(df_list, ignore_index=True)\n\n        return concatenated_df\n\n    def get_category_names_from_url(self, df, url_column):\n        \"\"\"\n        Extract the category names from the URL and create a new column in the DataFrame\n        \"\"\"\n        # Check if the URL is empty or None\n        mask = df[url_column].notnull()\n\n        # Split the URL into parts based on the \"/\" character\n        parts = df.loc[mask, url_column].str.split(\"/\", expand=True)\n\n        # Get the last part of the URL (after the last \"/\")\n        last_part = parts.iloc[:, -1]\n        # Split the last part of the URL on the \".\" character\n        last_part_parts = last_part.str.split(\".\", expand=True)\n\n        # Get the first part of the last part of the URL\n        category_name = last_part_parts.loc[:, 0]\n\n        # Create a new column in the DataFrame and assign the category names\n        df.loc[mask, \"category_name\"] = category_name\n\n        return df\n\n    def format_categorical_values(self, df, column, replacements):\n        \"\"\"\n        Replace column values with new values.\n        \"\"\"\n        df[column] = df[column].replace(replacements)\n        print(f\"\\nUnique values in the column '{column}' after formatting:\")\n        # Return rows with the replaced values.\n        replaced_rows = df.loc[\n            df[column].isin(replacements.values()),\n            [\"name\", \"hall\", \"stand\", \"category_name\"],\n        ]\n        print(f\"Rows with the replaced values: {replaced_rows}\")\n        return df\n\n    def remove_duplicates(self, df):\n        \"\"\"\n        Remove duplicates from the DataFrame\n        \"\"\"\n        # Check for duplicates\n        print(f\"Number of duplicates: {df.duplicated().sum()}\")\n        # Print rows with duplicates\n        print(df[df.duplicated()])\n        df.drop_duplicates(inplace=True)\n        # Reset the index\n        df.reset_index(drop=True, inplace=True)\n        # Check for duplicates again\n        print(f\"Number of duplicates after removing: {df.duplicated().sum()}\")\n        return df\n\n    def split_and_explode(self, df, columns, separator):\n        \"\"\"\n        Split columns using a separator and explode the resulting lists.\n        \"\"\"\n        # Check if the values for the columns have values with the separator. Print if True and return the rows with the values.\n        print(f\"Are there any values with the separator '{separator}' in the columns?\")\n        for column in columns:\n            if df[column].str.contains(separator).any():\n                print(\n                    f\"\\nYes, there are values with the separator '{separator}' in the column '{column}'. The rows are:/n\"\n                )\n                print(\n                    df[[\"name\", \"hall\", \"stand\"]].loc[\n                        df[column].str.contains(separator, na=False)\n                    ]\n                )\n\n                # Apply split to the specified column\n                print(\n                    f\"\\nSplitting the values in the column '{column}' using the separator '{separator}'.\"\n                )\n                df[column] = df[column].str.split(separator)\n\n                # Explode the DataFrame on the specified column\n                print(\n                    f\"\\nExploding the DataFrame on the column '{column}'. Then, stripping leading and trailing whitespace from the values.\"\n                )\n                df_exploded = df.explode(column)\n\n                # Strip leading and trailing whitespace from the values\n                df_exploded[column] = df_exploded[column].str.strip()\n\n                # Print the column values to check if the split and explode operations were successful.\n                print(f\"\\nColumn '{column}' values after splitting and exploding:\")\n                print(df_exploded[column].unique())\n\n                # Assign the exploded DataFrame back to df\n                df = df_exploded\n\n            else:\n                print(\n                    f\"\\nNo, there are no values with the separator '{separator}' in the columns.\"\n                )\n                pass\n\n        return df\n\n    def select_rearrange_columns(self, df, columns):\n        \"\"\"\n        Reorder the columns in the DataFrame.\n        \"\"\"\n        columns = [\"name\", \"category_name\", \"country\", \"hall\", \"stand\"]\n        df = df[columns]\n\n        return df\n</code></pre>"},{"location":"scripts/#app.scripts.transformer.Transformer.concatenate_csv_files","title":"<code>concatenate_csv_files(raw_data)</code>","text":"<p>Concatenate csv files. Set the working directory to the data directory to map the csv files.</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>def concatenate_csv_files(self, raw_data):\n    \"\"\"\n    Concatenate csv files. Set the working directory to the data directory to map the csv files.\n    \"\"\"\n    # Set the pattern to match all CSV files\n    pattern = f\"{raw_data}/*.csv\"\n    # Get a list of all CSV files in the working directory\n    csv_files = glob.glob(pattern)\n\n    # Create a list to store the DataFrames\n    df_list = []\n\n    # Loop through the list of CSV files\n    for csv_file in csv_files:\n        # Read each CSV into a DataFrame and append to the list\n        df = pd.read_csv(csv_file)\n        df_list.append(df)\n\n    concatenated_df = pd.concat(df_list, ignore_index=True)\n\n    return concatenated_df\n</code></pre>"},{"location":"scripts/#app.scripts.transformer.Transformer.format_categorical_values","title":"<code>format_categorical_values(df, column, replacements)</code>","text":"<p>Replace column values with new values.</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>def format_categorical_values(self, df, column, replacements):\n    \"\"\"\n    Replace column values with new values.\n    \"\"\"\n    df[column] = df[column].replace(replacements)\n    print(f\"\\nUnique values in the column '{column}' after formatting:\")\n    # Return rows with the replaced values.\n    replaced_rows = df.loc[\n        df[column].isin(replacements.values()),\n        [\"name\", \"hall\", \"stand\", \"category_name\"],\n    ]\n    print(f\"Rows with the replaced values: {replaced_rows}\")\n    return df\n</code></pre>"},{"location":"scripts/#app.scripts.transformer.Transformer.get_category_names_from_url","title":"<code>get_category_names_from_url(df, url_column)</code>","text":"<p>Extract the category names from the URL and create a new column in the DataFrame</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>def get_category_names_from_url(self, df, url_column):\n    \"\"\"\n    Extract the category names from the URL and create a new column in the DataFrame\n    \"\"\"\n    # Check if the URL is empty or None\n    mask = df[url_column].notnull()\n\n    # Split the URL into parts based on the \"/\" character\n    parts = df.loc[mask, url_column].str.split(\"/\", expand=True)\n\n    # Get the last part of the URL (after the last \"/\")\n    last_part = parts.iloc[:, -1]\n    # Split the last part of the URL on the \".\" character\n    last_part_parts = last_part.str.split(\".\", expand=True)\n\n    # Get the first part of the last part of the URL\n    category_name = last_part_parts.loc[:, 0]\n\n    # Create a new column in the DataFrame and assign the category names\n    df.loc[mask, \"category_name\"] = category_name\n\n    return df\n</code></pre>"},{"location":"scripts/#app.scripts.transformer.Transformer.remove_duplicates","title":"<code>remove_duplicates(df)</code>","text":"<p>Remove duplicates from the DataFrame</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>def remove_duplicates(self, df):\n    \"\"\"\n    Remove duplicates from the DataFrame\n    \"\"\"\n    # Check for duplicates\n    print(f\"Number of duplicates: {df.duplicated().sum()}\")\n    # Print rows with duplicates\n    print(df[df.duplicated()])\n    df.drop_duplicates(inplace=True)\n    # Reset the index\n    df.reset_index(drop=True, inplace=True)\n    # Check for duplicates again\n    print(f\"Number of duplicates after removing: {df.duplicated().sum()}\")\n    return df\n</code></pre>"},{"location":"scripts/#app.scripts.transformer.Transformer.select_rearrange_columns","title":"<code>select_rearrange_columns(df, columns)</code>","text":"<p>Reorder the columns in the DataFrame.</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>def select_rearrange_columns(self, df, columns):\n    \"\"\"\n    Reorder the columns in the DataFrame.\n    \"\"\"\n    columns = [\"name\", \"category_name\", \"country\", \"hall\", \"stand\"]\n    df = df[columns]\n\n    return df\n</code></pre>"},{"location":"scripts/#app.scripts.transformer.Transformer.split_and_explode","title":"<code>split_and_explode(df, columns, separator)</code>","text":"<p>Split columns using a separator and explode the resulting lists.</p> Source code in <code>app\\scripts\\transformer.py</code> <pre><code>def split_and_explode(self, df, columns, separator):\n    \"\"\"\n    Split columns using a separator and explode the resulting lists.\n    \"\"\"\n    # Check if the values for the columns have values with the separator. Print if True and return the rows with the values.\n    print(f\"Are there any values with the separator '{separator}' in the columns?\")\n    for column in columns:\n        if df[column].str.contains(separator).any():\n            print(\n                f\"\\nYes, there are values with the separator '{separator}' in the column '{column}'. The rows are:/n\"\n            )\n            print(\n                df[[\"name\", \"hall\", \"stand\"]].loc[\n                    df[column].str.contains(separator, na=False)\n                ]\n            )\n\n            # Apply split to the specified column\n            print(\n                f\"\\nSplitting the values in the column '{column}' using the separator '{separator}'.\"\n            )\n            df[column] = df[column].str.split(separator)\n\n            # Explode the DataFrame on the specified column\n            print(\n                f\"\\nExploding the DataFrame on the column '{column}'. Then, stripping leading and trailing whitespace from the values.\"\n            )\n            df_exploded = df.explode(column)\n\n            # Strip leading and trailing whitespace from the values\n            df_exploded[column] = df_exploded[column].str.strip()\n\n            # Print the column values to check if the split and explode operations were successful.\n            print(f\"\\nColumn '{column}' values after splitting and exploding:\")\n            print(df_exploded[column].unique())\n\n            # Assign the exploded DataFrame back to df\n            df = df_exploded\n\n        else:\n            print(\n                f\"\\nNo, there are no values with the separator '{separator}' in the columns.\"\n            )\n            pass\n\n    return df\n</code></pre>"},{"location":"workflow/","title":"Project Workflow","text":""},{"location":"workflow/#introduction","title":"Introduction","text":"<p>This section covers a high-level overview of the project workflow.</p>"},{"location":"workflow/#workflow","title":"Workflow","text":"<pre><code>flowchart TD\n    A([Extraction]) --&gt; B[Webpage]\n    B --&gt; C[Get n pages]\n    C --&gt; D[Iterate]\n    D --&gt; E{Error?}\n    E --&gt;|No| G[(Export page as csv)]\n    E --&gt;|Yes| F[Fix Error]\n    F --&gt; H[Identify + Exception]\n    H --&gt; E\n    G --&gt; I([Transform])\n    I --&gt; J(Concatenate csv files)\n    J --&gt; K(Format data)\n    K --&gt; L(Obtain categories) &amp; M(Rename values and columns)\n    L &amp; M --&gt; N(Explode and Split rows)\n    N --&gt; R([Load])\n    R --&gt; S([csv]) &amp; T([xlsx])\n    S &amp; T --&gt; U(Tableau)\n\n</code></pre>"}]}